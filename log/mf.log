2024-12-03 20:01:40,849 mf INFO #########mf###########
2024-12-03 20:01:40,849 mf INFO ########start training###########
2024-12-03 20:01:40,849 mf INFO epoch: 0
2024-12-03 20:01:40,849 mf INFO training
2024-12-03 20:01:48,420 mf INFO Epoch: 0 / 20, Step: 29 / 199, Loss(batch): 59648.7578125
2024-12-03 20:01:55,742 mf INFO Epoch: 0 / 20, Step: 59 / 199, Loss(batch): 24252.671875
2024-12-03 20:02:06,607 mf INFO Epoch: 0 / 20, Step: 89 / 199, Loss(batch): 6942.7880859375
2024-12-03 20:02:20,159 mf INFO Epoch: 0 / 20, Step: 119 / 199, Loss(batch): 234.37652587890625
2024-12-03 20:02:32,849 mf INFO Epoch: 0 / 20, Step: 149 / 199, Loss(batch): 74.05815124511719
2024-12-03 20:02:46,341 mf INFO Epoch: 0 / 20, Step: 179 / 199, Loss(batch): 90.02883911132812
2024-12-03 20:02:54,732 mf INFO epoch: 1
2024-12-03 20:02:54,733 mf INFO training
2024-12-03 20:03:08,594 mf INFO Epoch: 1 / 20, Step: 29 / 199, Loss(batch): 67.79765319824219
2024-12-03 20:03:20,554 mf INFO Epoch: 1 / 20, Step: 59 / 199, Loss(batch): 82.63674926757812
2024-12-03 20:03:33,221 mf INFO Epoch: 1 / 20, Step: 89 / 199, Loss(batch): 93.3135986328125
2024-12-03 20:03:46,319 mf INFO Epoch: 1 / 20, Step: 119 / 199, Loss(batch): 70.060302734375
2024-12-03 20:03:59,747 mf INFO Epoch: 1 / 20, Step: 149 / 199, Loss(batch): 70.28826904296875
2024-12-03 20:04:12,449 mf INFO Epoch: 1 / 20, Step: 179 / 199, Loss(batch): 77.1923599243164
2024-12-03 20:04:20,807 mf INFO epoch: 2
2024-12-03 20:04:20,810 mf INFO training
2024-12-03 20:04:33,779 mf INFO Epoch: 2 / 20, Step: 29 / 199, Loss(batch): 67.46760559082031
2024-12-03 20:04:46,754 mf INFO Epoch: 2 / 20, Step: 59 / 199, Loss(batch): 65.96265411376953
2024-12-03 20:04:58,485 mf INFO Epoch: 2 / 20, Step: 89 / 199, Loss(batch): 79.50238037109375
2024-12-03 20:05:12,851 mf INFO Epoch: 2 / 20, Step: 119 / 199, Loss(batch): 66.72930908203125
2024-12-03 20:05:26,081 mf INFO Epoch: 2 / 20, Step: 149 / 199, Loss(batch): 70.85539245605469
2024-12-03 20:05:36,273 mf INFO Epoch: 2 / 20, Step: 179 / 199, Loss(batch): 61.14867401123047
2024-12-03 20:05:42,898 mf INFO epoch: 3
2024-12-03 20:05:42,900 mf INFO training
2024-12-03 20:05:53,014 mf INFO Epoch: 3 / 20, Step: 29 / 199, Loss(batch): 78.29413604736328
2024-12-03 20:06:03,231 mf INFO Epoch: 3 / 20, Step: 59 / 199, Loss(batch): 63.8782844543457
2024-12-03 20:06:13,381 mf INFO Epoch: 3 / 20, Step: 89 / 199, Loss(batch): 76.98490905761719
2024-12-03 20:06:23,422 mf INFO Epoch: 3 / 20, Step: 119 / 199, Loss(batch): 70.62812042236328
2024-12-03 20:06:34,908 mf INFO Epoch: 3 / 20, Step: 149 / 199, Loss(batch): 87.13082122802734
2024-12-03 20:06:45,211 mf INFO Epoch: 3 / 20, Step: 179 / 199, Loss(batch): 70.92913818359375
2024-12-03 20:06:51,824 mf INFO validating
2024-12-03 20:06:54,510 mf INFO Valid Step: 9 / 57, Loss(batch): 80.90629577636719
2024-12-03 20:06:57,252 mf INFO Valid Step: 19 / 57, Loss(batch): 75.75083923339844
2024-12-03 20:07:00,962 mf INFO Valid Step: 29 / 57, Loss(batch): 84.1444091796875
2024-12-03 20:07:03,717 mf INFO Valid Step: 39 / 57, Loss(batch): 81.81411743164062
2024-12-03 20:07:07,099 mf INFO Valid Step: 49 / 57, Loss(batch): 81.64008331298828
2024-12-03 20:10:14,982 mf INFO ########valid metric###########
2024-12-03 20:10:14,983 mf INFO epoch3, train_loss75.82901311519757, valid_loss:74.84040096349884
2024-12-03 20:10:14,983 mf INFO threshold:0.52, f_score0.30641403043243914, auc0.7900952272996966, recall0.39243840271877656, precision0.251322859280419, aupr0.2630735632026025
2024-12-03 20:10:14,983 mf INFO epoch: 4
2024-12-03 20:10:14,983 mf INFO training
2024-12-03 20:10:25,204 mf INFO Epoch: 4 / 20, Step: 29 / 199, Loss(batch): 68.77886962890625
2024-12-03 20:10:37,576 mf INFO Epoch: 4 / 20, Step: 59 / 199, Loss(batch): 79.66950988769531
2024-12-03 20:10:49,552 mf INFO Epoch: 4 / 20, Step: 89 / 199, Loss(batch): 84.7444076538086
2024-12-03 20:11:00,414 mf INFO Epoch: 4 / 20, Step: 119 / 199, Loss(batch): 81.53815460205078
2024-12-03 20:11:12,045 mf INFO Epoch: 4 / 20, Step: 149 / 199, Loss(batch): 67.53570556640625
2024-12-03 20:11:22,654 mf INFO Epoch: 4 / 20, Step: 179 / 199, Loss(batch): 77.62454223632812
2024-12-03 20:11:30,064 mf INFO epoch: 5
2024-12-03 20:11:30,065 mf INFO training
2024-12-03 20:11:42,357 mf INFO Epoch: 5 / 20, Step: 29 / 199, Loss(batch): 80.89936065673828
2024-12-03 20:11:54,269 mf INFO Epoch: 5 / 20, Step: 59 / 199, Loss(batch): 72.54801940917969
2024-12-03 20:12:03,447 mf INFO Epoch: 5 / 20, Step: 89 / 199, Loss(batch): 84.4964599609375
2024-12-03 20:12:13,896 mf INFO Epoch: 5 / 20, Step: 119 / 199, Loss(batch): 76.89323425292969
2024-12-03 20:12:24,600 mf INFO Epoch: 5 / 20, Step: 149 / 199, Loss(batch): 286.1282653808594
2024-12-03 20:12:36,318 mf INFO Epoch: 5 / 20, Step: 179 / 199, Loss(batch): 74.63716888427734
2024-12-03 20:12:42,801 mf INFO epoch: 6
2024-12-03 20:12:42,802 mf INFO training
2024-12-03 20:12:52,223 mf INFO Epoch: 6 / 20, Step: 29 / 199, Loss(batch): 75.45696258544922
2024-12-03 20:13:02,258 mf INFO Epoch: 6 / 20, Step: 59 / 199, Loss(batch): 74.12079620361328
2024-12-03 20:13:11,421 mf INFO Epoch: 6 / 20, Step: 89 / 199, Loss(batch): 71.79273986816406
2024-12-03 20:13:21,798 mf INFO Epoch: 6 / 20, Step: 119 / 199, Loss(batch): 74.51166534423828
2024-12-03 20:13:32,448 mf INFO Epoch: 6 / 20, Step: 149 / 199, Loss(batch): 86.05776977539062
2024-12-03 20:13:45,410 mf INFO Epoch: 6 / 20, Step: 179 / 199, Loss(batch): 64.62133026123047
2024-12-03 20:13:53,668 mf INFO epoch: 7
2024-12-03 20:13:53,669 mf INFO training
2024-12-03 20:14:06,416 mf INFO Epoch: 7 / 20, Step: 29 / 199, Loss(batch): 74.25081634521484
2024-12-03 20:14:19,688 mf INFO Epoch: 7 / 20, Step: 59 / 199, Loss(batch): 71.005859375
2024-12-03 20:14:32,467 mf INFO Epoch: 7 / 20, Step: 89 / 199, Loss(batch): 75.84203338623047
2024-12-03 20:14:43,877 mf INFO Epoch: 7 / 20, Step: 119 / 199, Loss(batch): 67.5428695678711
2024-12-03 20:14:57,090 mf INFO Epoch: 7 / 20, Step: 149 / 199, Loss(batch): 78.65808868408203
2024-12-03 20:15:10,101 mf INFO Epoch: 7 / 20, Step: 179 / 199, Loss(batch): 77.50115966796875
2024-12-03 20:15:18,132 mf INFO validating
2024-12-03 20:15:21,918 mf INFO Valid Step: 9 / 57, Loss(batch): 76.61895751953125
2024-12-03 20:15:25,586 mf INFO Valid Step: 19 / 57, Loss(batch): 64.96967315673828
2024-12-03 20:15:29,621 mf INFO Valid Step: 29 / 57, Loss(batch): 81.53192901611328
2024-12-03 20:15:33,348 mf INFO Valid Step: 39 / 57, Loss(batch): 78.77243041992188
2024-12-03 20:15:37,178 mf INFO Valid Step: 49 / 57, Loss(batch): 69.79462432861328
2024-12-03 20:18:17,185 mf INFO ########valid metric###########
2024-12-03 20:18:17,185 mf INFO epoch7, train_loss74.63664883944257, valid_loss:74.4972441823859
2024-12-03 20:18:17,185 mf INFO threshold:0.55, f_score0.3799819249887031, auc0.8173434709587015, recall0.35722175021240443, precision0.4058397683397683, aupr0.4056235059719667
2024-12-03 20:18:17,185 mf INFO epoch: 8
2024-12-03 20:18:17,186 mf INFO training
2024-12-03 20:18:25,839 mf INFO Epoch: 8 / 20, Step: 29 / 199, Loss(batch): 69.80949401855469
2024-12-03 20:18:35,125 mf INFO Epoch: 8 / 20, Step: 59 / 199, Loss(batch): 63.51236343383789
2024-12-03 20:18:45,662 mf INFO Epoch: 8 / 20, Step: 89 / 199, Loss(batch): 70.68731689453125
2024-12-03 20:18:56,461 mf INFO Epoch: 8 / 20, Step: 119 / 199, Loss(batch): 67.84169006347656
2024-12-03 20:19:05,628 mf INFO Epoch: 8 / 20, Step: 149 / 199, Loss(batch): 78.55257415771484
2024-12-03 20:19:15,882 mf INFO Epoch: 8 / 20, Step: 179 / 199, Loss(batch): 72.52140808105469
2024-12-03 20:19:21,309 mf INFO epoch: 9
2024-12-03 20:19:21,310 mf INFO training
2024-12-03 20:19:31,512 mf INFO Epoch: 9 / 20, Step: 29 / 199, Loss(batch): 80.53837585449219
2024-12-03 20:19:41,884 mf INFO Epoch: 9 / 20, Step: 59 / 199, Loss(batch): 69.92152404785156
2024-12-03 20:19:51,337 mf INFO Epoch: 9 / 20, Step: 89 / 199, Loss(batch): 85.654296875
2024-12-03 20:20:01,565 mf INFO Epoch: 9 / 20, Step: 119 / 199, Loss(batch): 72.75791931152344
2024-12-03 20:20:11,091 mf INFO Epoch: 9 / 20, Step: 149 / 199, Loss(batch): 61.625823974609375
2024-12-03 20:20:21,328 mf INFO Epoch: 9 / 20, Step: 179 / 199, Loss(batch): 69.53298950195312
2024-12-03 20:20:27,888 mf INFO epoch: 10
2024-12-03 20:20:27,889 mf INFO training
2024-12-03 20:20:38,182 mf INFO Epoch: 10 / 20, Step: 29 / 199, Loss(batch): 56.87904739379883
2024-12-03 20:20:48,213 mf INFO Epoch: 10 / 20, Step: 59 / 199, Loss(batch): 79.3936767578125
2024-12-03 20:20:57,583 mf INFO Epoch: 10 / 20, Step: 89 / 199, Loss(batch): 76.07394409179688
2024-12-03 20:21:07,637 mf INFO Epoch: 10 / 20, Step: 119 / 199, Loss(batch): 75.41743469238281
2024-12-03 20:21:17,034 mf INFO Epoch: 10 / 20, Step: 149 / 199, Loss(batch): 68.14605712890625
2024-12-03 20:21:27,286 mf INFO Epoch: 10 / 20, Step: 179 / 199, Loss(batch): 70.66703796386719
2024-12-03 20:21:33,475 mf INFO epoch: 11
2024-12-03 20:21:33,476 mf INFO training
2024-12-03 20:21:42,529 mf INFO Epoch: 11 / 20, Step: 29 / 199, Loss(batch): 81.89228820800781
2024-12-03 20:21:53,995 mf INFO Epoch: 11 / 20, Step: 59 / 199, Loss(batch): 62.48963928222656
2024-12-03 20:22:03,418 mf INFO Epoch: 11 / 20, Step: 89 / 199, Loss(batch): 60.55961608886719
2024-12-03 20:22:12,382 mf INFO Epoch: 11 / 20, Step: 119 / 199, Loss(batch): 82.72289276123047
2024-12-03 20:22:23,190 mf INFO Epoch: 11 / 20, Step: 149 / 199, Loss(batch): 75.79647827148438
2024-12-03 20:22:33,229 mf INFO Epoch: 11 / 20, Step: 179 / 199, Loss(batch): 68.2268295288086
2024-12-03 20:22:39,820 mf INFO validating
2024-12-03 20:22:42,613 mf INFO Valid Step: 9 / 57, Loss(batch): 81.3316879272461
2024-12-03 20:22:45,148 mf INFO Valid Step: 19 / 57, Loss(batch): 65.49456787109375
2024-12-03 20:22:47,779 mf INFO Valid Step: 29 / 57, Loss(batch): 71.30870056152344
2024-12-03 20:22:50,774 mf INFO Valid Step: 39 / 57, Loss(batch): 67.86058807373047
2024-12-03 20:22:54,023 mf INFO Valid Step: 49 / 57, Loss(batch): 76.44429016113281
2024-12-03 20:25:34,205 mf INFO ########valid metric###########
2024-12-03 20:25:34,206 mf INFO epoch11, train_loss73.69702756105356, valid_loss:74.36582759388706
2024-12-03 20:25:34,206 mf INFO threshold:0.57, f_score0.3780774994647827, auc0.82240166630705, recall0.33759558198810535, precision0.4295907886912806, aupr0.42209764409143696
2024-12-03 20:25:34,206 mf INFO epoch: 12
2024-12-03 20:25:34,206 mf INFO training
2024-12-03 20:25:46,199 mf INFO Epoch: 12 / 20, Step: 29 / 199, Loss(batch): 70.28489685058594
2024-12-03 20:26:00,068 mf INFO Epoch: 12 / 20, Step: 59 / 199, Loss(batch): 83.10391235351562
2024-12-03 20:26:13,082 mf INFO Epoch: 12 / 20, Step: 89 / 199, Loss(batch): 77.54011535644531
2024-12-03 20:26:24,528 mf INFO Epoch: 12 / 20, Step: 119 / 199, Loss(batch): 70.786376953125
2024-12-03 20:26:36,912 mf INFO Epoch: 12 / 20, Step: 149 / 199, Loss(batch): 69.66761779785156
2024-12-03 20:26:49,187 mf INFO Epoch: 12 / 20, Step: 179 / 199, Loss(batch): 70.20358276367188
2024-12-03 20:26:56,733 mf INFO epoch: 13
2024-12-03 20:26:56,734 mf INFO training
2024-12-03 20:27:07,253 mf INFO Epoch: 13 / 20, Step: 29 / 199, Loss(batch): 63.096954345703125
2024-12-03 20:27:16,225 mf INFO Epoch: 13 / 20, Step: 59 / 199, Loss(batch): 76.82550048828125
2024-12-03 20:27:26,804 mf INFO Epoch: 13 / 20, Step: 89 / 199, Loss(batch): 64.15235900878906
2024-12-03 20:27:36,905 mf INFO Epoch: 13 / 20, Step: 119 / 199, Loss(batch): 74.2149887084961
2024-12-03 20:27:45,947 mf INFO Epoch: 13 / 20, Step: 149 / 199, Loss(batch): 65.0263671875
2024-12-03 20:27:56,847 mf INFO Epoch: 13 / 20, Step: 179 / 199, Loss(batch): 68.96653747558594
2024-12-03 20:28:03,362 mf INFO epoch: 14
2024-12-03 20:28:03,364 mf INFO training
2024-12-03 20:28:13,417 mf INFO Epoch: 14 / 20, Step: 29 / 199, Loss(batch): 71.28204345703125
2024-12-03 20:28:22,501 mf INFO Epoch: 14 / 20, Step: 59 / 199, Loss(batch): 66.91197204589844
2024-12-03 20:28:34,026 mf INFO Epoch: 14 / 20, Step: 89 / 199, Loss(batch): 65.56005859375
2024-12-03 20:28:44,904 mf INFO Epoch: 14 / 20, Step: 119 / 199, Loss(batch): 74.90337371826172
2024-12-03 20:28:54,917 mf INFO Epoch: 14 / 20, Step: 149 / 199, Loss(batch): 72.01154327392578
2024-12-03 20:29:05,364 mf INFO Epoch: 14 / 20, Step: 179 / 199, Loss(batch): 72.80815124511719
2024-12-03 20:29:11,146 mf INFO epoch: 15
2024-12-03 20:29:11,148 mf INFO training
2024-12-03 20:29:21,382 mf INFO Epoch: 15 / 20, Step: 29 / 199, Loss(batch): 73.73892974853516
2024-12-03 20:29:31,487 mf INFO Epoch: 15 / 20, Step: 59 / 199, Loss(batch): 72.92701721191406
2024-12-03 20:29:42,043 mf INFO Epoch: 15 / 20, Step: 89 / 199, Loss(batch): 78.74127197265625
2024-12-03 20:29:51,316 mf INFO Epoch: 15 / 20, Step: 119 / 199, Loss(batch): 70.20866394042969
2024-12-03 20:30:01,335 mf INFO Epoch: 15 / 20, Step: 149 / 199, Loss(batch): 86.32810974121094
2024-12-03 20:30:11,018 mf INFO Epoch: 15 / 20, Step: 179 / 199, Loss(batch): 65.3446273803711
2024-12-03 20:30:16,643 mf INFO validating
2024-12-03 20:30:19,581 mf INFO Valid Step: 9 / 57, Loss(batch): 76.01776123046875
2024-12-03 20:30:21,530 mf INFO Valid Step: 19 / 57, Loss(batch): 71.3440170288086
2024-12-03 20:30:24,403 mf INFO Valid Step: 29 / 57, Loss(batch): 75.96974182128906
2024-12-03 20:30:27,303 mf INFO Valid Step: 39 / 57, Loss(batch): 70.6265869140625
2024-12-03 20:30:30,282 mf INFO Valid Step: 49 / 57, Loss(batch): 67.24067687988281
2024-12-03 20:33:09,329 mf INFO ########valid metric###########
2024-12-03 20:33:09,330 mf INFO epoch15, train_loss73.30305139742904, valid_loss:74.1913345069216
2024-12-03 20:33:09,330 mf INFO threshold:0.57, f_score0.38216762972969903, auc0.8239515325291916, recall0.3575191163976211, precision0.4104665057185359, aupr0.42470151663054867
2024-12-03 20:33:09,330 mf INFO epoch: 16
2024-12-03 20:33:09,330 mf INFO training
2024-12-03 20:33:23,246 mf INFO Epoch: 16 / 20, Step: 29 / 199, Loss(batch): 56.95537567138672
2024-12-03 20:33:35,917 mf INFO Epoch: 16 / 20, Step: 59 / 199, Loss(batch): 76.50863647460938
2024-12-03 20:33:50,850 mf INFO Epoch: 16 / 20, Step: 89 / 199, Loss(batch): 63.99869155883789
2024-12-03 20:34:03,680 mf INFO Epoch: 16 / 20, Step: 119 / 199, Loss(batch): 90.44486999511719
2024-12-03 20:34:15,439 mf INFO Epoch: 16 / 20, Step: 149 / 199, Loss(batch): 66.32191467285156
2024-12-03 20:34:29,233 mf INFO Epoch: 16 / 20, Step: 179 / 199, Loss(batch): 79.36418914794922
2024-12-03 20:34:37,658 mf INFO epoch: 17
2024-12-03 20:34:37,658 mf INFO training
2024-12-03 20:34:49,424 mf INFO Epoch: 17 / 20, Step: 29 / 199, Loss(batch): 63.87561798095703
2024-12-03 20:35:03,056 mf INFO Epoch: 17 / 20, Step: 59 / 199, Loss(batch): 75.46504211425781
2024-12-03 20:35:16,809 mf INFO Epoch: 17 / 20, Step: 89 / 199, Loss(batch): 81.62565612792969
2024-12-03 20:35:29,464 mf INFO Epoch: 17 / 20, Step: 119 / 199, Loss(batch): 69.24595642089844
2024-12-03 20:35:42,674 mf INFO Epoch: 17 / 20, Step: 149 / 199, Loss(batch): 67.56177520751953
2024-12-03 20:35:56,211 mf INFO Epoch: 17 / 20, Step: 179 / 199, Loss(batch): 69.57466888427734
2024-12-03 20:36:04,464 mf INFO epoch: 18
2024-12-03 20:36:04,464 mf INFO training
2024-12-03 20:36:17,360 mf INFO Epoch: 18 / 20, Step: 29 / 199, Loss(batch): 65.25759887695312
2024-12-03 20:36:30,118 mf INFO Epoch: 18 / 20, Step: 59 / 199, Loss(batch): 67.60371398925781
2024-12-03 20:36:42,934 mf INFO Epoch: 18 / 20, Step: 89 / 199, Loss(batch): 75.05526733398438
2024-12-03 20:36:54,396 mf INFO Epoch: 18 / 20, Step: 119 / 199, Loss(batch): 68.94515991210938
2024-12-03 20:37:06,469 mf INFO Epoch: 18 / 20, Step: 149 / 199, Loss(batch): 77.15391540527344
2024-12-03 20:37:18,692 mf INFO Epoch: 18 / 20, Step: 179 / 199, Loss(batch): 87.28337097167969
2024-12-03 20:37:25,464 mf INFO epoch: 19
2024-12-03 20:37:25,465 mf INFO training
2024-12-03 20:37:36,799 mf INFO Epoch: 19 / 20, Step: 29 / 199, Loss(batch): 67.20187377929688
2024-12-03 20:37:45,954 mf INFO Epoch: 19 / 20, Step: 59 / 199, Loss(batch): 86.72234344482422
2024-12-03 20:37:56,199 mf INFO Epoch: 19 / 20, Step: 89 / 199, Loss(batch): 77.51324462890625
2024-12-03 20:38:07,730 mf INFO Epoch: 19 / 20, Step: 119 / 199, Loss(batch): 69.98428344726562
2024-12-03 20:38:17,119 mf INFO Epoch: 19 / 20, Step: 149 / 199, Loss(batch): 62.56727600097656
2024-12-03 20:38:27,438 mf INFO Epoch: 19 / 20, Step: 179 / 199, Loss(batch): 82.47261810302734
2024-12-03 20:38:33,085 mf INFO validating
2024-12-03 20:38:36,673 mf INFO Valid Step: 9 / 57, Loss(batch): 70.19271850585938
2024-12-03 20:38:40,360 mf INFO Valid Step: 19 / 57, Loss(batch): 76.57794952392578
2024-12-03 20:38:43,379 mf INFO Valid Step: 29 / 57, Loss(batch): 78.85320281982422
2024-12-03 20:38:46,327 mf INFO Valid Step: 39 / 57, Loss(batch): 79.26237487792969
2024-12-03 20:38:49,159 mf INFO Valid Step: 49 / 57, Loss(batch): 85.08393096923828
2024-12-03 20:41:27,449 mf INFO ########valid metric###########
2024-12-03 20:41:27,450 mf INFO epoch19, train_loss73.16136008411196, valid_loss:74.19369587145354
2024-12-03 20:41:27,450 mf INFO threshold:0.57, f_score0.3787048553915801, auc0.8234990247963305, recall0.3589209855564996, precision0.400796945043998, aupr0.4105638040315391
2024-12-03 20:41:27,450 mf INFO best_fscore: 0.38216762972969903
2024-12-03 20:41:27,450 mf INFO best_scores[thresh,fmax,recall,precision,auc]: [0.57, 0.38216762972969903, 0.3575191163976211, 0.4104665057185359, 0.8239515325291916]
2024-12-03 20:41:27,450 mf INFO best_aupr: 0.42470151663054867
2024-12-03 20:41:27,450 mf INFO best_score_dict: {0.01: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.02: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.03: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.04: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.05: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.06: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.07: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.08: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.09: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.1: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.11: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.12: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.13: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.14: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.15: [0.07607536437966583, 1.0, 0.03954175905395418, 0.8239515325291916], 0.16: [0.07607542584383047, 1.0, 0.03954179226448991, 0.8239515325291916], 0.17: [0.07607542584383047, 1.0, 0.03954179226448991, 0.8239515325291916], 0.18: [0.07607542584383047, 1.0, 0.03954179226448991, 0.8239515325291916], 0.19: [0.07607548730809441, 1.0, 0.03954182547508143, 0.8239515325291916], 0.2: [0.07607561023692026, 1.0, 0.03954189189643182, 0.8239515325291916], 0.21: [0.07607579463090393, 1.0, 0.03954199152887582, 0.8239515325291916], 0.22: [0.07607610195619645, 1.0, 0.03954215758406488, 0.8239515325291916], 0.23: [0.07607622488700871, 1.0, 0.03954222400653102, 0.8239515325291916], 0.24: [0.07607647074982508, 1.0, 0.039542356852132754, 0.8239515325291916], 0.25: [0.07607690101357766, 1.0, 0.039542589334083646, 0.8239515325291916], 0.26: [0.07607714688076427, 1.0, 0.0395427221821401, 0.8239515325291916], 0.27: [0.07607745421698234, 1.0, 0.03954288824346593, 0.8239515325291916], 0.28: [0.07607831477160346, 1.0, 0.039543353222598505, 0.8239515325291916], 0.32: [0.07607950872656755, 0.9999575191163976, 0.039544064778416155, 0.8239515325291916], 0.33: [0.07608004428050244, 0.9999362786745964, 0.03954438737153345, 0.8239515325291916], 0.34: [0.07608201155039906, 0.9999362786745964, 0.039545450345372726, 0.8239515325291916], 0.35: [0.0760841457422875, 0.9999150382327953, 0.03954663673815012, 0.8239515325291916], 0.36: [0.07608726396987218, 0.9998937977909941, 0.039548354847246316, 0.8239515325291916], 0.37: [0.07609098006759979, 0.9998513169073917, 0.03955042925486598, 0.8239515325291916], 0.38: [0.076098527805822, 0.9998300764655905, 0.03955454084818071, 0.8239515325291916], 0.39: [0.07610736236496024, 0.9997238742565845, 0.03955948084432558, 0.8239515325291916], 0.4: [0.07631814583630725, 0.999320305862362, 0.039674026701206885, 0.8239515325291916], 0.41: [0.07687477846150571, 0.9973237043330502, 0.0399781692665163, 0.8239515325291916], 0.42: [0.07805214038411507, 0.9940739167374681, 0.04062079045674846, 0.8239515325291916], 0.43: [0.08060149018307393, 0.9860662701784197, 0.04201803126009744, 0.8239515325291916], 0.44: [0.08459543721741186, 0.9728547153780799, 0.044220327331989405, 0.8239515325291916], 0.45: [0.09254445059804756, 0.9512107051826678, 0.048638263302401556, 0.8239515325291916], 0.46: [0.10432884579823859, 0.9167587085811385, 0.05531171066340477, 0.8239515325291916], 0.47: [0.12277575020578653, 0.8728122344944775, 0.06603214189872136, 0.8239515325291916], 0.48: [0.14633715367393704, 0.8267629566694987, 0.08027271771118402, 0.8239515325291916], 0.49: [0.17705823052181843, 0.783411214953271, 0.09980786924284245, 0.8239515325291916], 0.5: [0.20480581818181817, 0.7476847918436703, 0.1186537229918765, 0.8239515325291916], 0.51: [0.2316504684353384, 0.7024426508071367, 0.13869445784143095, 0.8239515325291916], 0.52: [0.25248025585797274, 0.6573067119796092, 0.15624873773074283, 0.8239515325291916], 0.53: [0.27503866611339545, 0.6194562446898896, 0.1767600853374709, 0.8239515325291916], 0.54: [0.3121086917948848, 0.5230671197960918, 0.22240887251183122, 0.8239515325291916], 0.55: [0.3608083560399637, 0.43876380628717077, 0.306374584717608, 0.8239515325291916], 0.56: [0.37015733378446963, 0.41826677994902295, 0.331973431336188, 0.8239515325291916], 0.57: [0.38216762972969903, 0.3575191163976211, 0.4104665057185359, 0.8239515325291916]}
