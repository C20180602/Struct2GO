2024-12-03 20:01:56,185 cc INFO #########cc###########
2024-12-03 20:01:56,185 cc INFO ########start training###########
2024-12-03 20:01:56,186 cc INFO epoch: 0
2024-12-03 20:01:56,186 cc INFO training
2024-12-03 20:02:08,395 cc INFO Epoch: 0 / 20, Step: 29 / 205, Loss(batch): 55786.57421875
2024-12-03 20:02:21,741 cc INFO Epoch: 0 / 20, Step: 59 / 205, Loss(batch): 12694.53125
2024-12-03 20:02:34,457 cc INFO Epoch: 0 / 20, Step: 89 / 205, Loss(batch): 25046.328125
2024-12-03 20:02:47,020 cc INFO Epoch: 0 / 20, Step: 119 / 205, Loss(batch): 6291.72509765625
2024-12-03 20:03:00,351 cc INFO Epoch: 0 / 20, Step: 149 / 205, Loss(batch): 1597.47021484375
2024-12-03 20:03:14,176 cc INFO Epoch: 0 / 20, Step: 179 / 205, Loss(batch): 871.8917236328125
2024-12-03 20:03:24,285 cc INFO epoch: 1
2024-12-03 20:03:24,286 cc INFO training
2024-12-03 20:03:36,997 cc INFO Epoch: 1 / 20, Step: 29 / 205, Loss(batch): 115.5846939086914
2024-12-03 20:03:49,178 cc INFO Epoch: 1 / 20, Step: 59 / 205, Loss(batch): 102.18990325927734
2024-12-03 20:04:03,063 cc INFO Epoch: 1 / 20, Step: 89 / 205, Loss(batch): 105.89734649658203
2024-12-03 20:04:16,170 cc INFO Epoch: 1 / 20, Step: 119 / 205, Loss(batch): 101.61976623535156
2024-12-03 20:04:28,730 cc INFO Epoch: 1 / 20, Step: 149 / 205, Loss(batch): 95.87313842773438
2024-12-03 20:04:42,702 cc INFO Epoch: 1 / 20, Step: 179 / 205, Loss(batch): 119.32512664794922
2024-12-03 20:04:52,377 cc INFO epoch: 2
2024-12-03 20:04:52,378 cc INFO training
2024-12-03 20:05:06,009 cc INFO Epoch: 2 / 20, Step: 29 / 205, Loss(batch): 106.16886901855469
2024-12-03 20:05:18,890 cc INFO Epoch: 2 / 20, Step: 59 / 205, Loss(batch): 103.29985809326172
2024-12-03 20:05:30,377 cc INFO Epoch: 2 / 20, Step: 89 / 205, Loss(batch): 96.38882446289062
2024-12-03 20:05:40,498 cc INFO Epoch: 2 / 20, Step: 119 / 205, Loss(batch): 111.1657943725586
2024-12-03 20:05:51,589 cc INFO Epoch: 2 / 20, Step: 149 / 205, Loss(batch): 94.41415405273438
2024-12-03 20:06:01,635 cc INFO Epoch: 2 / 20, Step: 179 / 205, Loss(batch): 98.07662963867188
2024-12-03 20:06:10,149 cc INFO epoch: 3
2024-12-03 20:06:10,150 cc INFO training
2024-12-03 20:06:20,328 cc INFO Epoch: 3 / 20, Step: 29 / 205, Loss(batch): 91.53115844726562
2024-12-03 20:06:30,488 cc INFO Epoch: 3 / 20, Step: 59 / 205, Loss(batch): 110.144287109375
2024-12-03 20:06:40,571 cc INFO Epoch: 3 / 20, Step: 89 / 205, Loss(batch): 87.70175170898438
2024-12-03 20:06:51,907 cc INFO Epoch: 3 / 20, Step: 119 / 205, Loss(batch): 116.96167755126953
2024-12-03 20:07:01,337 cc INFO Epoch: 3 / 20, Step: 149 / 205, Loss(batch): 101.79032897949219
2024-12-03 20:07:10,058 cc INFO Epoch: 3 / 20, Step: 179 / 205, Loss(batch): 106.44145202636719
2024-12-03 20:07:15,754 cc INFO validating
2024-12-03 20:07:18,245 cc INFO Valid Step: 9 / 59, Loss(batch): 97.99707794189453
2024-12-03 20:07:19,889 cc INFO Valid Step: 19 / 59, Loss(batch): 98.25437927246094
2024-12-03 20:07:21,064 cc INFO Valid Step: 29 / 59, Loss(batch): 106.9830551147461
2024-12-03 20:07:23,068 cc INFO Valid Step: 39 / 59, Loss(batch): 88.23516845703125
2024-12-03 20:07:23,966 cc INFO Valid Step: 49 / 59, Loss(batch): 93.06608581542969
2024-12-03 20:10:18,344 cc INFO ########valid metric###########
2024-12-03 20:10:18,345 cc INFO epoch3, train_loss102.85400476223084, valid_loss:100.6672175779181
2024-12-03 20:10:18,345 cc INFO threshold:0.52, f_score0.3014602869629452, auc0.7900090605727805, recall0.6187374899172468, precision0.19927548433343115, aupr0.23473629520423353
2024-12-03 20:10:18,345 cc INFO epoch: 4
2024-12-03 20:10:18,345 cc INFO training
2024-12-03 20:10:31,552 cc INFO Epoch: 4 / 20, Step: 29 / 205, Loss(batch): 106.58796691894531
2024-12-03 20:10:43,766 cc INFO Epoch: 4 / 20, Step: 59 / 205, Loss(batch): 102.68807983398438
2024-12-03 20:10:56,181 cc INFO Epoch: 4 / 20, Step: 89 / 205, Loss(batch): 101.5492172241211
2024-12-03 20:11:06,843 cc INFO Epoch: 4 / 20, Step: 119 / 205, Loss(batch): 105.95083618164062
2024-12-03 20:11:17,401 cc INFO Epoch: 4 / 20, Step: 149 / 205, Loss(batch): 110.92896270751953
2024-12-03 20:11:29,430 cc INFO Epoch: 4 / 20, Step: 179 / 205, Loss(batch): 95.47232818603516
2024-12-03 20:11:39,004 cc INFO epoch: 5
2024-12-03 20:11:39,005 cc INFO training
2024-12-03 20:11:51,359 cc INFO Epoch: 5 / 20, Step: 29 / 205, Loss(batch): 108.89629364013672
2024-12-03 20:12:02,131 cc INFO Epoch: 5 / 20, Step: 59 / 205, Loss(batch): 110.31951904296875
2024-12-03 20:12:12,860 cc INFO Epoch: 5 / 20, Step: 89 / 205, Loss(batch): 99.92724609375
2024-12-03 20:12:23,239 cc INFO Epoch: 5 / 20, Step: 119 / 205, Loss(batch): 111.20292663574219
2024-12-03 20:12:33,898 cc INFO Epoch: 5 / 20, Step: 149 / 205, Loss(batch): 106.54389190673828
2024-12-03 20:12:45,337 cc INFO Epoch: 5 / 20, Step: 179 / 205, Loss(batch): 95.67369079589844
2024-12-03 20:12:52,249 cc INFO epoch: 6
2024-12-03 20:12:52,250 cc INFO training
2024-12-03 20:13:02,271 cc INFO Epoch: 6 / 20, Step: 29 / 205, Loss(batch): 94.82472229003906
2024-12-03 20:13:12,720 cc INFO Epoch: 6 / 20, Step: 59 / 205, Loss(batch): 111.30957794189453
2024-12-03 20:13:22,316 cc INFO Epoch: 6 / 20, Step: 89 / 205, Loss(batch): 101.75851440429688
2024-12-03 20:13:33,057 cc INFO Epoch: 6 / 20, Step: 119 / 205, Loss(batch): 98.50364685058594
2024-12-03 20:13:45,890 cc INFO Epoch: 6 / 20, Step: 149 / 205, Loss(batch): 104.96527862548828
2024-12-03 20:13:58,345 cc INFO Epoch: 6 / 20, Step: 179 / 205, Loss(batch): 97.85307312011719
2024-12-03 20:14:09,113 cc INFO epoch: 7
2024-12-03 20:14:09,114 cc INFO training
2024-12-03 20:14:22,740 cc INFO Epoch: 7 / 20, Step: 29 / 205, Loss(batch): 117.00437927246094
2024-12-03 20:14:35,782 cc INFO Epoch: 7 / 20, Step: 59 / 205, Loss(batch): 111.3280029296875
2024-12-03 20:14:46,625 cc INFO Epoch: 7 / 20, Step: 89 / 205, Loss(batch): 108.18666076660156
2024-12-03 20:14:59,375 cc INFO Epoch: 7 / 20, Step: 119 / 205, Loss(batch): 106.71470642089844
2024-12-03 20:15:13,410 cc INFO Epoch: 7 / 20, Step: 149 / 205, Loss(batch): 89.9385986328125
2024-12-03 20:15:26,029 cc INFO Epoch: 7 / 20, Step: 179 / 205, Loss(batch): 112.30597686767578
2024-12-03 20:15:36,135 cc INFO validating
2024-12-03 20:15:39,564 cc INFO Valid Step: 9 / 59, Loss(batch): 97.30429077148438
2024-12-03 20:15:42,244 cc INFO Valid Step: 19 / 59, Loss(batch): 108.00320434570312
2024-12-03 20:15:45,634 cc INFO Valid Step: 29 / 59, Loss(batch): 95.18058776855469
2024-12-03 20:15:48,791 cc INFO Valid Step: 39 / 59, Loss(batch): 93.95907592773438
2024-12-03 20:15:51,692 cc INFO Valid Step: 49 / 59, Loss(batch): 103.77609252929688
2024-12-03 20:18:19,849 cc INFO ########valid metric###########
2024-12-03 20:18:19,850 cc INFO epoch7, train_loss100.50777725591892, valid_loss:99.96314950716697
2024-12-03 20:18:19,850 cc INFO threshold:0.55, f_score0.39120815602483483, auc0.8326716637830843, recall0.5863681175873092, precision0.29351727231942576, aupr0.3195003688887177
2024-12-03 20:18:19,850 cc INFO epoch: 8
2024-12-03 20:18:19,850 cc INFO training
2024-12-03 20:18:30,128 cc INFO Epoch: 8 / 20, Step: 29 / 205, Loss(batch): 91.01271057128906
2024-12-03 20:18:40,570 cc INFO Epoch: 8 / 20, Step: 59 / 205, Loss(batch): 99.36624145507812
2024-12-03 20:18:50,981 cc INFO Epoch: 8 / 20, Step: 89 / 205, Loss(batch): 107.67803955078125
2024-12-03 20:19:01,199 cc INFO Epoch: 8 / 20, Step: 119 / 205, Loss(batch): 106.42466735839844
2024-12-03 20:19:11,436 cc INFO Epoch: 8 / 20, Step: 149 / 205, Loss(batch): 102.57664489746094
2024-12-03 20:19:20,551 cc INFO Epoch: 8 / 20, Step: 179 / 205, Loss(batch): 106.28826904296875
2024-12-03 20:19:28,715 cc INFO epoch: 9
2024-12-03 20:19:28,716 cc INFO training
2024-12-03 20:19:38,903 cc INFO Epoch: 9 / 20, Step: 29 / 205, Loss(batch): 91.99812316894531
2024-12-03 20:19:49,424 cc INFO Epoch: 9 / 20, Step: 59 / 205, Loss(batch): 103.349365234375
2024-12-03 20:19:58,710 cc INFO Epoch: 9 / 20, Step: 89 / 205, Loss(batch): 109.23097229003906
2024-12-03 20:20:08,719 cc INFO Epoch: 9 / 20, Step: 119 / 205, Loss(batch): 102.48017883300781
2024-12-03 20:20:19,133 cc INFO Epoch: 9 / 20, Step: 149 / 205, Loss(batch): 90.8632583618164
2024-12-03 20:20:28,675 cc INFO Epoch: 9 / 20, Step: 179 / 205, Loss(batch): 101.87350463867188
2024-12-03 20:20:37,496 cc INFO epoch: 10
2024-12-03 20:20:37,499 cc INFO training
2024-12-03 20:20:46,938 cc INFO Epoch: 10 / 20, Step: 29 / 205, Loss(batch): 87.54147338867188
2024-12-03 20:20:57,001 cc INFO Epoch: 10 / 20, Step: 59 / 205, Loss(batch): 103.73765563964844
2024-12-03 20:21:07,088 cc INFO Epoch: 10 / 20, Step: 89 / 205, Loss(batch): 96.09274291992188
2024-12-03 20:21:16,268 cc INFO Epoch: 10 / 20, Step: 119 / 205, Loss(batch): 101.1233901977539
2024-12-03 20:21:27,007 cc INFO Epoch: 10 / 20, Step: 149 / 205, Loss(batch): 92.70454406738281
2024-12-03 20:21:36,828 cc INFO Epoch: 10 / 20, Step: 179 / 205, Loss(batch): 106.48907470703125
2024-12-03 20:21:45,003 cc INFO epoch: 11
2024-12-03 20:21:45,004 cc INFO training
2024-12-03 20:21:55,437 cc INFO Epoch: 11 / 20, Step: 29 / 205, Loss(batch): 112.96367645263672
2024-12-03 20:22:05,945 cc INFO Epoch: 11 / 20, Step: 59 / 205, Loss(batch): 109.06083679199219
2024-12-03 20:22:15,313 cc INFO Epoch: 11 / 20, Step: 89 / 205, Loss(batch): 85.31637573242188
2024-12-03 20:22:25,502 cc INFO Epoch: 11 / 20, Step: 119 / 205, Loss(batch): 87.51634979248047
2024-12-03 20:22:35,674 cc INFO Epoch: 11 / 20, Step: 149 / 205, Loss(batch): 85.09320068359375
2024-12-03 20:22:44,375 cc INFO Epoch: 11 / 20, Step: 179 / 205, Loss(batch): 93.5609130859375
2024-12-03 20:22:52,749 cc INFO validating
2024-12-03 20:22:55,053 cc INFO Valid Step: 9 / 59, Loss(batch): 95.64098358154297
2024-12-03 20:22:56,924 cc INFO Valid Step: 19 / 59, Loss(batch): 88.81105041503906
2024-12-03 20:22:58,974 cc INFO Valid Step: 29 / 59, Loss(batch): 99.32769775390625
2024-12-03 20:23:00,965 cc INFO Valid Step: 39 / 59, Loss(batch): 97.9728775024414
2024-12-03 20:23:02,125 cc INFO Valid Step: 49 / 59, Loss(batch): 96.90312957763672
2024-12-03 20:25:31,314 cc INFO ########valid metric###########
2024-12-03 20:25:31,316 cc INFO epoch11, train_loss97.1022871808308, valid_loss:98.8619736493644
2024-12-03 20:25:31,316 cc INFO threshold:0.69, f_score0.3110395951094057, auc0.8061546371580025, recall0.33782451528097274, precision0.28819001987868903, aupr0.3214046593732737
2024-12-03 20:25:31,316 cc INFO epoch: 12
2024-12-03 20:25:31,316 cc INFO training
2024-12-03 20:25:44,744 cc INFO Epoch: 12 / 20, Step: 29 / 205, Loss(batch): 82.57075500488281
2024-12-03 20:25:57,620 cc INFO Epoch: 12 / 20, Step: 59 / 205, Loss(batch): 88.96698760986328
2024-12-03 20:26:10,611 cc INFO Epoch: 12 / 20, Step: 89 / 205, Loss(batch): 95.30708312988281
2024-12-03 20:26:23,050 cc INFO Epoch: 12 / 20, Step: 119 / 205, Loss(batch): 99.11080932617188
2024-12-03 20:26:35,307 cc INFO Epoch: 12 / 20, Step: 149 / 205, Loss(batch): 103.3924789428711
2024-12-03 20:26:48,442 cc INFO Epoch: 12 / 20, Step: 179 / 205, Loss(batch): 110.86152648925781
2024-12-03 20:26:58,022 cc INFO epoch: 13
2024-12-03 20:26:58,025 cc INFO training
2024-12-03 20:27:07,965 cc INFO Epoch: 13 / 20, Step: 29 / 205, Loss(batch): 97.48159790039062
2024-12-03 20:27:17,089 cc INFO Epoch: 13 / 20, Step: 59 / 205, Loss(batch): 81.5731201171875
2024-12-03 20:27:27,839 cc INFO Epoch: 13 / 20, Step: 89 / 205, Loss(batch): 98.88076782226562
2024-12-03 20:27:37,615 cc INFO Epoch: 13 / 20, Step: 119 / 205, Loss(batch): 90.3409652709961
2024-12-03 20:27:46,502 cc INFO Epoch: 13 / 20, Step: 149 / 205, Loss(batch): 93.03767395019531
2024-12-03 20:27:56,893 cc INFO Epoch: 13 / 20, Step: 179 / 205, Loss(batch): 101.99740600585938
2024-12-03 20:28:04,911 cc INFO epoch: 14
2024-12-03 20:28:04,913 cc INFO training
2024-12-03 20:28:16,295 cc INFO Epoch: 14 / 20, Step: 29 / 205, Loss(batch): 104.35086059570312
2024-12-03 20:28:25,583 cc INFO Epoch: 14 / 20, Step: 59 / 205, Loss(batch): 103.66375732421875
2024-12-03 20:28:36,803 cc INFO Epoch: 14 / 20, Step: 89 / 205, Loss(batch): 83.0698013305664
2024-12-03 20:28:47,314 cc INFO Epoch: 14 / 20, Step: 119 / 205, Loss(batch): 92.00749206542969
2024-12-03 20:28:57,326 cc INFO Epoch: 14 / 20, Step: 149 / 205, Loss(batch): 100.30854797363281
2024-12-03 20:29:06,904 cc INFO Epoch: 14 / 20, Step: 179 / 205, Loss(batch): 100.56302642822266
2024-12-03 20:29:14,631 cc INFO epoch: 15
2024-12-03 20:29:14,632 cc INFO training
2024-12-03 20:29:24,963 cc INFO Epoch: 15 / 20, Step: 29 / 205, Loss(batch): 96.33255767822266
2024-12-03 20:29:35,339 cc INFO Epoch: 15 / 20, Step: 59 / 205, Loss(batch): 103.82456970214844
2024-12-03 20:29:45,637 cc INFO Epoch: 15 / 20, Step: 89 / 205, Loss(batch): 112.86964416503906
2024-12-03 20:29:55,893 cc INFO Epoch: 15 / 20, Step: 119 / 205, Loss(batch): 99.50689697265625
2024-12-03 20:30:05,367 cc INFO Epoch: 15 / 20, Step: 149 / 205, Loss(batch): 106.01112365722656
2024-12-03 20:30:15,640 cc INFO Epoch: 15 / 20, Step: 179 / 205, Loss(batch): 92.7988510131836
2024-12-03 20:30:21,824 cc INFO validating
2024-12-03 20:30:24,689 cc INFO Valid Step: 9 / 59, Loss(batch): 91.71620178222656
2024-12-03 20:30:27,718 cc INFO Valid Step: 19 / 59, Loss(batch): 88.07683563232422
2024-12-03 20:30:30,595 cc INFO Valid Step: 29 / 59, Loss(batch): 93.1823501586914
2024-12-03 20:30:33,058 cc INFO Valid Step: 39 / 59, Loss(batch): 94.37492370605469
2024-12-03 20:30:35,204 cc INFO Valid Step: 49 / 59, Loss(batch): 93.11723327636719
2024-12-03 20:33:01,797 cc INFO ########valid metric###########
2024-12-03 20:33:01,798 cc INFO epoch15, train_loss94.82780441656345, valid_loss:97.88410303148173
2024-12-03 20:33:01,798 cc INFO threshold:0.76, f_score0.38692847124824686, auc0.8196029708010947, recall0.41209332895169243, precision0.3646601633753668, aupr0.3968264478518591
2024-12-03 20:33:01,798 cc INFO epoch: 16
2024-12-03 20:33:01,798 cc INFO training
2024-12-03 20:33:13,112 cc INFO Epoch: 16 / 20, Step: 29 / 205, Loss(batch): 73.31856536865234
2024-12-03 20:33:27,544 cc INFO Epoch: 16 / 20, Step: 59 / 205, Loss(batch): 105.69619750976562
2024-12-03 20:33:40,729 cc INFO Epoch: 16 / 20, Step: 89 / 205, Loss(batch): 113.44847106933594
2024-12-03 20:33:54,874 cc INFO Epoch: 16 / 20, Step: 119 / 205, Loss(batch): 103.75096130371094
2024-12-03 20:34:07,965 cc INFO Epoch: 16 / 20, Step: 149 / 205, Loss(batch): 76.20440673828125
2024-12-03 20:34:20,952 cc INFO Epoch: 16 / 20, Step: 179 / 205, Loss(batch): 101.14563751220703
2024-12-03 20:34:30,968 cc INFO epoch: 17
2024-12-03 20:34:30,970 cc INFO training
2024-12-03 20:34:43,420 cc INFO Epoch: 17 / 20, Step: 29 / 205, Loss(batch): 94.03943634033203
2024-12-03 20:34:56,764 cc INFO Epoch: 17 / 20, Step: 59 / 205, Loss(batch): 92.51838684082031
2024-12-03 20:35:11,417 cc INFO Epoch: 17 / 20, Step: 89 / 205, Loss(batch): 99.64148712158203
2024-12-03 20:35:24,356 cc INFO Epoch: 17 / 20, Step: 119 / 205, Loss(batch): 80.799560546875
2024-12-03 20:35:35,748 cc INFO Epoch: 17 / 20, Step: 149 / 205, Loss(batch): 108.03923034667969
2024-12-03 20:35:48,711 cc INFO Epoch: 17 / 20, Step: 179 / 205, Loss(batch): 94.60450744628906
2024-12-03 20:35:59,951 cc INFO epoch: 18
2024-12-03 20:35:59,953 cc INFO training
2024-12-03 20:36:13,078 cc INFO Epoch: 18 / 20, Step: 29 / 205, Loss(batch): 81.6380615234375
2024-12-03 20:36:26,043 cc INFO Epoch: 18 / 20, Step: 59 / 205, Loss(batch): 94.34696960449219
2024-12-03 20:36:38,930 cc INFO Epoch: 18 / 20, Step: 89 / 205, Loss(batch): 109.861572265625
2024-12-03 20:36:51,415 cc INFO Epoch: 18 / 20, Step: 119 / 205, Loss(batch): 102.68356323242188
2024-12-03 20:37:02,902 cc INFO Epoch: 18 / 20, Step: 149 / 205, Loss(batch): 91.40065002441406
2024-12-03 20:37:15,660 cc INFO Epoch: 18 / 20, Step: 179 / 205, Loss(batch): 86.88121032714844
2024-12-03 20:37:25,465 cc INFO epoch: 19
2024-12-03 20:37:25,466 cc INFO training
2024-12-03 20:37:37,077 cc INFO Epoch: 19 / 20, Step: 29 / 205, Loss(batch): 89.71459197998047
2024-12-03 20:37:47,818 cc INFO Epoch: 19 / 20, Step: 59 / 205, Loss(batch): 88.44332885742188
2024-12-03 20:37:57,762 cc INFO Epoch: 19 / 20, Step: 89 / 205, Loss(batch): 96.37730407714844
2024-12-03 20:38:08,485 cc INFO Epoch: 19 / 20, Step: 119 / 205, Loss(batch): 102.73129272460938
2024-12-03 20:38:17,926 cc INFO Epoch: 19 / 20, Step: 149 / 205, Loss(batch): 101.23428344726562
2024-12-03 20:38:28,588 cc INFO Epoch: 19 / 20, Step: 179 / 205, Loss(batch): 87.75447082519531
2024-12-03 20:38:36,010 cc INFO validating
2024-12-03 20:38:40,175 cc INFO Valid Step: 9 / 59, Loss(batch): 96.00555419921875
2024-12-03 20:38:43,206 cc INFO Valid Step: 19 / 59, Loss(batch): 98.53515625
2024-12-03 20:38:46,329 cc INFO Valid Step: 29 / 59, Loss(batch): 90.06019592285156
2024-12-03 20:38:49,153 cc INFO Valid Step: 39 / 59, Loss(batch): 81.1694107055664
2024-12-03 20:38:52,118 cc INFO Valid Step: 49 / 59, Loss(batch): 98.36658477783203
2024-12-03 20:41:18,103 cc INFO ########valid metric###########
2024-12-03 20:41:18,104 cc INFO epoch19, train_loss93.9595835615949, valid_loss:97.84233532921742
2024-12-03 20:41:18,104 cc INFO threshold:0.75, f_score0.3947600866195578, auc0.8245429018608648, recall0.41390075583305946, precision0.37731147362400935, aupr0.40626328131527367
2024-12-03 20:41:18,104 cc INFO best_fscore: 0.3947600866195578
2024-12-03 20:41:18,104 cc INFO best_scores[thresh,fmax,recall,precision,auc]: [0.75, 0.3947600866195578, 0.41390075583305946, 0.37731147362400935, 0.8245429018608648]
2024-12-03 20:41:18,104 cc INFO best_aupr: 0.40626328131527367
2024-12-03 20:41:18,104 cc INFO best_score_dict: {0.01: [0.11861330196109003, 1.0, 0.06304567906466453, 0.8245429018608648], 0.02: [0.11861908153755107, 1.0, 0.06304894472645765, 0.8245429018608648], 0.03: [0.11863022194657334, 1.0, 0.06305523950178205, 0.8245429018608648], 0.04: [0.11864474366302022, 0.9999850625877573, 0.06306350436955807, 0.8245429018608648], 0.05: [0.11866883994579211, 0.9999701251755146, 0.06307717965557433, 0.8245429018608648], 0.06: [0.11870399628335186, 0.9999551877632719, 0.06309710524133043, 0.8245429018608648], 0.07: [0.11874192907519618, 0.9999253129387865, 0.06311866017700116, 0.8245429018608648], 0.08: [0.1187929920706472, 0.9998655632898157, 0.06314775576742018, 0.8245429018608648], 0.09: [0.118852389877747, 0.9997610014041167, 0.06318174309111935, 0.8245429018608648], 0.1: [0.11893017754844351, 0.9996265646939324, 0.06322624775966827, 0.8245429018608648], 0.11: [0.11902830551167656, 0.9994921279837481, 0.06328225634080416, 0.8245429018608648], 0.12: [0.1191422215969097, 0.9992830042123503, 0.06334749935608988, 0.8245429018608648], 0.13: [0.11928911604963811, 0.9991635049144086, 0.063431041856522, 0.8245429018608648], 0.14: [0.11945563480843714, 0.9990290682042243, 0.06352575993738703, 0.8245429018608648], 0.15: [0.11964752080867294, 0.9987751321960984, 0.06363533405600069, 0.8245429018608648], 0.16: [0.11986444446436736, 0.9985510710124578, 0.06375898457171635, 0.8245429018608648], 0.17: [0.12011280121212775, 0.998237385355361, 0.06390083074522283, 0.8245429018608648], 0.18: [0.12042464222630764, 0.9980282615839632, 0.06407824347315703, 0.8245429018608648], 0.19: [0.12075755548403196, 0.9975801392166821, 0.06426865344160256, 0.8245429018608648], 0.2: [0.12113821138211382, 0.9971021420249156, 0.06448633703462547, 0.8245429018608648], 0.21: [0.12157042819423718, 0.996579332596421, 0.06473356361096018, 0.8245429018608648], 0.22: [0.12203590282017741, 0.9957129626863442, 0.06500127254625292, 0.8245429018608648], 0.23: [0.12257904229084618, 0.9949362172497237, 0.06531289038981618, 0.8245429018608648], 0.24: [0.12318919054005648, 0.9940250351029187, 0.06566341799867975, 0.8245429018608648], 0.25: [0.12387453059832818, 0.9931287903683566, 0.0660569600445109, 0.8245429018608648], 0.26: [0.12465092186655456, 0.9921130463358527, 0.06650325717569924, 0.8245429018608648], 0.27: [0.12549334947874294, 0.9907537418217668, 0.06698925578266746, 0.8245429018608648], 0.28: [0.12643209613719483, 0.9891405012995549, 0.06753202516528871, 0.8245429018608648], 0.29: [0.12747674040761778, 0.9874077614794013, 0.06813667534914596, 0.8245429018608648], 0.3: [0.12863623868503704, 0.9855853971857915, 0.06880847797703221, 0.8245429018608648], 0.31: [0.12987279482418773, 0.9832103486392018, 0.06952842107042206, 0.8245429018608648], 0.32: [0.13128749353969005, 0.98088011232934, 0.07035193107297784, 0.8245429018608648], 0.33: [0.13280979117115693, 0.9783855644848086, 0.0712400969315171, 0.8245429018608648], 0.34: [0.13449680046538684, 0.9756221432199086, 0.07222690976004352, 0.8245429018608648], 0.35: [0.13632674348826468, 0.9722164132285723, 0.07330272201931058, 0.8245429018608648], 0.36: [0.138361264307735, 0.9687210587637798, 0.07450108215659335, 0.8245429018608648], 0.37: [0.14064118522645172, 0.9653750784214142, 0.07584538001319091, 0.8245429018608648], 0.38: [0.14311539419382382, 0.9614166641770979, 0.07731198424042954, 0.8245429018608648], 0.39: [0.14586860357162362, 0.9575478744062379, 0.07894756287700959, 0.8245429018608648], 0.4: [0.14896588741153888, 0.9534998356884653, 0.08079420911338124, 0.8245429018608648], 0.41: [0.15233956209211463, 0.9487646760075285, 0.08281872613856842, 0.8245429018608648], 0.42: [0.15586773617790758, 0.9437755803184656, 0.08494864655194435, 0.8245429018608648], 0.43: [0.1601054250840839, 0.9382487377886655, 0.0875200470400049, 0.8245429018608648], 0.44: [0.16573998908747323, 0.9301675977653632, 0.09097509949100344, 0.8245429018608648], 0.45: [0.1735282107605007, 0.9204284049831207, 0.09579413099850134, 0.8245429018608648], 0.46: [0.1829512695671014, 0.9081647895318615, 0.10172162826883502, 0.8245429018608648], 0.47: [0.19226828012771274, 0.8954679891255639, 0.1076959835872672, 0.8245429018608648], 0.48: [0.20306091689705363, 0.8804558898216474, 0.11476463155107389, 0.8245429018608648], 0.49: [0.21240771825901675, 0.866638783497147, 0.12103650204550813, 0.8245429018608648], 0.5: [0.22097562296648496, 0.8531951124787142, 0.12692437602218587, 0.8245429018608648], 0.51: [0.22980220534603338, 0.8400501897051355, 0.13310737726064553, 0.8245429018608648], 0.52: [0.23806655105790622, 0.8264123323275476, 0.13906344258998593, 0.8245429018608648], 0.53: [0.2462579496542739, 0.8088907477668569, 0.14523686278506542, 0.8245429018608648], 0.54: [0.2551813595964449, 0.7934155886834164, 0.1520406235508968, 0.8245429018608648], 0.55: [0.26393499203134674, 0.7767603740328025, 0.1589768204024482, 0.8245429018608648], 0.56: [0.27299067978043556, 0.7599707226720043, 0.16637779928840518, 0.8245429018608648], 0.57: [0.28218498743183973, 0.7428673856541093, 0.17417312245212446, 0.8245429018608648], 0.58: [0.28959627329192544, 0.7187434648821438, 0.1813286202036494, 0.8245429018608648], 0.59: [0.2980504853874019, 0.6986974576524363, 0.18942845456349447, 0.8245429018608648], 0.6: [0.3061696744813628, 0.6778896423983509, 0.1977394729503625, 0.8245429018608648], 0.61: [0.31490941215864804, 0.6602485585397185, 0.20676315395554226, 0.8245429018608648], 0.62: [0.3219646424667345, 0.6379320646491202, 0.21531783164602913, 0.8245429018608648], 0.63: [0.3276212049307486, 0.6135691452812715, 0.22347353474024384, 0.8245429018608648], 0.64: [0.334606486350061, 0.5941056971290294, 0.2328848135656736, 0.8245429018608648], 0.65: [0.34392213818877415, 0.5800944044453739, 0.24441437472465227, 0.8245429018608648], 0.66: [0.3457244027822605, 0.5520120694290921, 0.25167362449518854, 0.8245429018608648], 0.67: [0.35358409451994827, 0.5368804708272339, 0.2635913607861831, 0.8245429018608648], 0.68: [0.3615688382574896, 0.5224509306007827, 0.27644204170025766, 0.8245429018608648], 0.69: [0.36869957491107835, 0.5078720162519045, 0.2893961833750404, 0.8245429018608648], 0.7: [0.3756945628089313, 0.49387566098049174, 0.3031522775617986, 0.8245429018608648], 0.71: [0.3814256299170426, 0.4786992501419054, 0.31700826969493134, 0.8245429018608648], 0.72: [0.38701200032382815, 0.4641502106175126, 0.33185949398182263, 0.8245429018608648], 0.73: [0.39113607339258255, 0.44834642846473277, 0.34687391656073036, 0.8245429018608648], 0.74: [0.392947034548617, 0.43025722223881935, 0.3615912828431188, 0.8245429018608648], 0.75: [0.3947600866195578, 0.41390075583305946, 0.37731147362400935, 0.8245429018608648]}
